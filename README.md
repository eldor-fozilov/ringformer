# RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals

**Jaemu Heo\*, Eldor Fozilov\*, Hyunmin Song, Taehwan Kim**  
IMML Lab, UNIST  
ðŸ“§ {skek000, eldorfozilov, hyunminsong, taehwankim}@unist.ac.kr  

[*Equal contribution*]  

---

## ðŸ“– Overview
**RingFormer** is a novel recurrent Transformer model that reduces parameters while maintaining high performance in machine translation and image classification.  

**Contributions:**  
    
    âœ…  We enhance a recurrent Transformer architecture to significantly reduce the model's parameter count while maintaining high performance.

    âœ…  We propose novel input-dependent level signals generated in a parameter-efficient way using low-rank matrices to improve the adaptability of a recurrent Transformer model, and show that those signals help the model replicate the behavior of the original model.

    âœ…  We demonstrate the validity of our approach through careful analysis and ablation studies, and show the effectiveness of our model on tasks such as translation and image classification.

For more details, check our paper:  
ðŸ“„ **[RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals]()**  



